{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5367d6c5-b847-4e6b-b165-0876d2a91181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, importlib, inspect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Had issues with the root bc of where i started the notebook from but used parents and grandparent levels to get to the actual repo root\n",
    "repo_root = Path.cwd()\n",
    "for up in [repo_root, repo_root.parent, repo_root.parent.parent]:\n",
    "    if (up / \"src\").exists():\n",
    "        repo_root = up\n",
    "        break\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "#importing our geo for coordinate translation\n",
    "import src.geo as geo \n",
    "importlib.invalidate_caches()\n",
    "geo = importlib.reload(geo)\n",
    "\n",
    "# clean name for the rest of the notebook\n",
    "latlonalt_to_enu = geo.latlonalt_to_enu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0f9e5f3-facf-4cfe-a36f-682ac635da8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>icao24</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>velocity</th>\n",
       "      <th>heading</th>\n",
       "      <th>vertrate</th>\n",
       "      <th>callsign</th>\n",
       "      <th>onground</th>\n",
       "      <th>alert</th>\n",
       "      <th>spi</th>\n",
       "      <th>squawk</th>\n",
       "      <th>baroaltitude</th>\n",
       "      <th>geoaltitude</th>\n",
       "      <th>lastposupdate</th>\n",
       "      <th>lastcontact</th>\n",
       "      <th>alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-12 00:00:00</td>\n",
       "      <td>4062d5</td>\n",
       "      <td>52.741870</td>\n",
       "      <td>-0.567287</td>\n",
       "      <td>172.119770</td>\n",
       "      <td>295.492795</td>\n",
       "      <td>-9.10336</td>\n",
       "      <td>EXS46B</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>9212.58</td>\n",
       "      <td>9372.60</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>9372.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-12 00:00:10</td>\n",
       "      <td>a9f4c6</td>\n",
       "      <td>38.733215</td>\n",
       "      <td>-90.131836</td>\n",
       "      <td>150.662723</td>\n",
       "      <td>83.530180</td>\n",
       "      <td>13.00480</td>\n",
       "      <td>3536</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>1905.00</td>\n",
       "      <td>1981.20</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>1981.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-12 00:00:10</td>\n",
       "      <td>a0b20e</td>\n",
       "      <td>35.164902</td>\n",
       "      <td>-104.443939</td>\n",
       "      <td>221.811287</td>\n",
       "      <td>268.405170</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>FDX556</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7677.0</td>\n",
       "      <td>11574.78</td>\n",
       "      <td>12146.28</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>12146.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-06-12 00:00:10</td>\n",
       "      <td>85c970</td>\n",
       "      <td>35.337385</td>\n",
       "      <td>138.482724</td>\n",
       "      <td>203.732815</td>\n",
       "      <td>264.203031</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>ANA791</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>7924.80</td>\n",
       "      <td>8161.02</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>8161.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-06-12 00:00:10</td>\n",
       "      <td>a0ce85</td>\n",
       "      <td>39.907278</td>\n",
       "      <td>-104.448242</td>\n",
       "      <td>130.236658</td>\n",
       "      <td>73.712637</td>\n",
       "      <td>8.12800</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>625.0</td>\n",
       "      <td>6515.10</td>\n",
       "      <td>3154.68</td>\n",
       "      <td>1.497225e+09</td>\n",
       "      <td>1.497226e+09</td>\n",
       "      <td>3154.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  icao24        lat         lon    velocity     heading  \\\n",
       "0 2017-06-12 00:00:00  4062d5  52.741870   -0.567287  172.119770  295.492795   \n",
       "1 2017-06-12 00:00:10  a9f4c6  38.733215  -90.131836  150.662723   83.530180   \n",
       "2 2017-06-12 00:00:10  a0b20e  35.164902 -104.443939  221.811287  268.405170   \n",
       "5 2017-06-12 00:00:10  85c970  35.337385  138.482724  203.732815  264.203031   \n",
       "6 2017-06-12 00:00:10  a0ce85  39.907278 -104.448242  130.236658   73.712637   \n",
       "\n",
       "   vertrate callsign  onground  alert    spi  squawk  baroaltitude  \\\n",
       "0  -9.10336   EXS46B     False  False  False  3244.0       9212.58   \n",
       "1  13.00480     3536     False  False  False  1734.0       1905.00   \n",
       "2   0.00000   FDX556     False  False  False  7677.0      11574.78   \n",
       "5   0.00000   ANA791     False  False  False  2355.0       7924.80   \n",
       "6   8.12800              False  False  False   625.0       6515.10   \n",
       "\n",
       "   geoaltitude  lastposupdate   lastcontact       alt  \n",
       "0      9372.60   1.497226e+09  1.497226e+09   9372.60  \n",
       "1      1981.20   1.497226e+09  1.497226e+09   1981.20  \n",
       "2     12146.28   1.497226e+09  1.497226e+09  12146.28  \n",
       "5      8161.02   1.497226e+09  1.497226e+09   8161.02  \n",
       "6      3154.68   1.497225e+09  1.497226e+09   3154.68  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used absolute path just because of my issues with the project root -- will fix them later\n",
    "DATA_PATH = \"/users/danielvillafuerte/weather-aware-trajectory-prediction/data/raw/06_12_17.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# time, callsign cleanup\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\", errors=\"coerce\")\n",
    "df[\"callsign\"] = df.get(\"callsign\", \"\").fillna(\"\").str.strip()\n",
    "\n",
    "# drop rows without position and time bc they aren't useful to us\n",
    "df = df.dropna(subset=[\"lat\",\"lon\",\"time\"]).copy()\n",
    "\n",
    "# altitude column - preffered geoaltitude but will use baroalitude as fallback just in case, will take a 0 as an extra fallback just to avoid NaNs\n",
    "if \"geoaltitude\" in df.columns:\n",
    "    df[\"alt\"] = df[\"geoaltitude\"]\n",
    "    if \"baroaltitude\" in df.columns:\n",
    "        df[\"alt\"] = df[\"alt\"].fillna(df[\"baroaltitude\"])\n",
    "elif \"baroaltitude\" in df.columns:\n",
    "    df[\"alt\"] = df[\"baroaltitude\"]\n",
    "else:\n",
    "    df[\"alt\"] = 0.0\n",
    "df[\"alt\"] = df[\"alt\"].fillna(0.0)\n",
    "\n",
    "#now lets check what we have so far\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c58b3de7-4f75-41bc-8b90-a68a88243e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort + flight segmentation\n",
    "df = df.sort_values([\"icao24\",\"time\"]).reset_index(drop=True)\n",
    "df[\"gap_s\"] = df.groupby(\"icao24\")[\"time\"].diff().dt.total_seconds().fillna(0)\n",
    "df[\"flight_id\"] = ((df[\"icao24\"] != df[\"icao24\"].shift()) | (df[\"gap_s\"] > 600)).cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10bc9c34-2c0a-4833-98ad-6b11107983c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_consecutive_runs(df, min_len=60, max_gap_s=120):\n",
    "    \"\"\"\n",
    "    Keep only continuous, valid segments within each flight:\n",
    "      - valid = lat, lon, alt present\n",
    "      - new run when invalid OR time gap > max_gap_s\n",
    "      - keep runs with at least min_len consecutive valid samples\n",
    "    \"\"\"\n",
    "    d = df.sort_values([\"flight_id\",\"time\"]).copy()\n",
    "    d[\"valid\"] = d[[\"lat\",\"lon\",\"alt\"]].notna().all(axis=1)\n",
    "    d[\"dt\"] = d.groupby(\"flight_id\")[\"time\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    break_flag = (~d[\"valid\"]) | (d[\"dt\"] > max_gap_s)\n",
    "    d[\"run_id\"] = break_flag.groupby(d[\"flight_id\"]).cumsum()\n",
    "\n",
    "    d_valid = d[d[\"valid\"]].copy()\n",
    "    run_sizes = d_valid.groupby([\"flight_id\",\"run_id\"]).size()\n",
    "    keeps = run_sizes[run_sizes >= min_len].reset_index()[[\"flight_id\",\"run_id\"]]\n",
    "    out = d_valid.merge(keeps, on=[\"flight_id\",\"run_id\"], how=\"inner\")\n",
    "    return out.drop(columns=[\"valid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "550e6356-05f4-423b-adf6-1b8f10907c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'flight_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/4j/b4y6wwhd0m18s4zqjd6p25mc0000gn/T/ipykernel_2896/887551126.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Keep continuous runs and sort after lets check how many runs we actually kept\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_runs = keep_consecutive_runs(df, min_len=\u001b[32m60\u001b[39m, max_gap_s=\u001b[32m120\u001b[39m).copy()\n\u001b[32m      3\u001b[39m df_runs = df_runs.sort_values([\u001b[33m\"flight_id\"\u001b[39m,\u001b[33m\"run_id\"\u001b[39m,\u001b[33m\"time\"\u001b[39m]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m print(\"Runs kept:\", df_runs.groupby([\"flight_id\",\"run_id\"]).size().shape[0],\n",
      "\u001b[32m/var/folders/4j/b4y6wwhd0m18s4zqjd6p25mc0000gn/T/ipykernel_2896/2086175550.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, min_len, max_gap_s)\u001b[39m\n\u001b[32m      4\u001b[39m       - valid = lat, lon, alt present\n\u001b[32m      5\u001b[39m       - new run when invalid OR time gap > max_gap_s\n\u001b[32m      6\u001b[39m       - keep runs \u001b[38;5;28;01mwith\u001b[39;00m at least min_len consecutive valid samples\n\u001b[32m      7\u001b[39m     \"\"\"\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     d = df.sort_values([\u001b[33m\"flight_id\"\u001b[39m,\u001b[33m\"time\"\u001b[39m]).copy()\n\u001b[32m      9\u001b[39m     d[\u001b[33m\"valid\"\u001b[39m] = d[[\u001b[33m\"lat\"\u001b[39m,\u001b[33m\"lon\"\u001b[39m,\u001b[33m\"alt\"\u001b[39m]].notna().all(axis=\u001b[32m1\u001b[39m)\n\u001b[32m     10\u001b[39m     d[\u001b[33m\"dt\"\u001b[39m] = d.groupby(\u001b[33m\"flight_id\"\u001b[39m)[\u001b[33m\"time\"\u001b[39m].diff().dt.total_seconds().fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     11\u001b[39m \n",
      "\u001b[32m~/weather-aware-trajectory-prediction/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7175\u001b[39m                 f\"Length of ascending ({len(ascending)})\"  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   7176\u001b[39m                 f\" != length of by ({len(by)})\"\n\u001b[32m   7177\u001b[39m             )\n\u001b[32m   7178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(by) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7179\u001b[39m             keys = [self._get_label_or_level_values(x, axis=axis) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m by]\n\u001b[32m   7180\u001b[39m \n\u001b[32m   7181\u001b[39m             \u001b[38;5;66;03m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[32m   7182\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/weather-aware-trajectory-prediction/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'flight_id'"
     ]
    }
   ],
   "source": [
    "# Keep continuous runs and sort after lets check how many runs we actually kept\n",
    "df_runs = keep_consecutive_runs(df, min_len=60, max_gap_s=120).copy()\n",
    "df_runs = df_runs.sort_values([\"flight_id\",\"run_id\",\"time\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Runs kept:\", df_runs.groupby([\"flight_id\",\"run_id\"]).size().shape[0],\n",
    "      \"| rows:\", len(df_runs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b24e44d-26f6-4ce1-a566-d761d05024ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENU per (flight_id, run_id), anchored at first sample of each run\n",
    "runs = []\n",
    "for (fid, rid), g in df_runs.groupby([\"flight_id\",\"run_id\"], sort=False):\n",
    "    g = g.sort_values(\"time\").copy()\n",
    "    lat0, lon0, alt0 = g.iloc[0][[\"lat\",\"lon\",\"alt\"]]\n",
    "    E, N, U = latlonalt_to_enu(\n",
    "        g[\"lat\"].to_numpy(),\n",
    "        g[\"lon\"].to_numpy(),\n",
    "        g[\"alt\"].to_numpy(),\n",
    "        float(lat0), float(lon0), float(alt0)\n",
    "    )\n",
    "    g[\"E\"], g[\"N\"], g[\"U\"] = E, N, U\n",
    "    runs.append(g)\n",
    "\n",
    "df_runs = pd.concat(runs, ignore_index=True)\n",
    "del runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ee1ec68-ca44-48a7-abf3-16dee2f86fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments kept: 5492 | median len: 167 | min/max: 60 / 359\n",
      "                 E            N            U           vE           vN  \\\n",
      "count  1038555.000  1038555.000  1038555.000  1033063.000  1033063.000   \n",
      "mean      3623.369    15954.238    -6349.379        4.077       11.715   \n",
      "std     243086.761   168533.944    13439.911      667.696      557.247   \n",
      "min   -2126501.613 -1423972.650  -574410.003  -222303.167  -152195.385   \n",
      "25%     -92555.121   -39988.059    -9364.080      -89.026      -26.400   \n",
      "50%          0.000      311.114    -1518.899        0.000        0.000   \n",
      "75%      98922.595    75155.142        3.388       91.582       65.925   \n",
      "max    1486336.529  1576440.127    37703.760   221471.841   153061.609   \n",
      "\n",
      "                vU        speed  turn_rate  \n",
      "count  1033063.000  1033063.000        0.0  \n",
      "mean        -6.171      167.402        NaN  \n",
      "std        131.525      863.602        NaN  \n",
      "min     -52703.771        0.000        NaN  \n",
      "25%        -10.672        0.000        NaN  \n",
      "50%         -2.151      170.622        NaN  \n",
      "75%          0.000      233.745        NaN  \n",
      "max      53132.049   274998.621        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Kinematics on native cadence - lets use the ENU we alraedy got to get vE, vN, VU and turn rate\n",
    "gb = df_runs.groupby([\"flight_id\",\"run_id\"], sort=False)\n",
    "\n",
    "df_runs[\"dt\"] = gb[\"time\"].diff().dt.total_seconds()\n",
    "df_runs[\"dt\"] = df_runs[\"dt\"].replace(0, np.nan)\n",
    "\n",
    "for c in [\"E\",\"N\",\"U\"]:\n",
    "    df_runs[f\"d{c}\"] = gb[c].diff()\n",
    "\n",
    "df_runs[\"vE\"] = df_runs[\"dE\"] / df_runs[\"dt\"]\n",
    "df_runs[\"vN\"] = df_runs[\"dN\"] / df_runs[\"dt\"]\n",
    "df_runs[\"vU\"] = df_runs[\"dU\"] / df_runs[\"dt\"]\n",
    "df_runs[\"speed\"] = np.sqrt(df_runs[\"vE\"]**2 + df_runs[\"vN\"]**2 + df_runs[\"vU\"]**2)\n",
    "\n",
    "df_runs[\"heading_rad\"] = np.arctan2(df_runs[\"vE\"], df_runs[\"vN\"])\n",
    "df_runs[\"heading_unwrapped\"] = gb[\"heading_rad\"].transform(np.unwrap)\n",
    "df_runs[\"turn_rate\"] = gb[\"heading_unwrapped\"].diff() / df_runs[\"dt\"]\n",
    "\n",
    "seg = gb.size().rename(\"len\").reset_index()\n",
    "print(\"Segments kept:\", len(seg),\n",
    "      \"| median len:\", int(seg[\"len\"].median()),\n",
    "      \"| min/max:\", int(seg[\"len\"].min()), \"/\", int(seg[\"len\"].max()))\n",
    "print(df_runs[[\"E\",\"N\",\"U\",\"vE\",\"vN\",\"vU\",\"speed\",\"turn_rate\"]].describe().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e3eec39-169b-4f03-b3fe-606a5006427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_rows          1.038555e+06\n",
      "E_nonzero_nonNaN    9.981860e+05\n",
      "E_pct               9.611296e+01\n",
      "N_nonzero_nonNaN    9.980340e+05\n",
      "N_pct               9.609833e+01\n",
      "U_nonzero_nonNaN    1.008920e+06\n",
      "U_pct               9.714652e+01\n",
      "dtype: float64\n",
      "total_rows                  1.038555e+06\n",
      "valid_rows                  1.038555e+06\n",
      "valid_pct                   1.000000e+02\n",
      "any_nonzero_rows            1.016795e+06\n",
      "any_nonzero_pct_of_valid    9.790478e+01\n",
      "all_nonzero_rows            9.909490e+05\n",
      "all_nonzero_pct_of_valid    9.541613e+01\n",
      "ignored_first_rows          5.335000e+03\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Quality summaries (same helpers you wrote)\n",
    "def enu_counts(df: pd.DataFrame, eps: float = 1e-6):\n",
    "    total = len(df)\n",
    "    out = {\"total_rows\": total}\n",
    "    for c in (\"E\",\"N\",\"U\"):\n",
    "        mask = df[c].notna() & (df[c].abs() > eps)\n",
    "        cnt = int(mask.sum())\n",
    "        out[f\"{c}_nonzero_nonNaN\"] = cnt\n",
    "        out[f\"{c}_pct\"] = (cnt/total*100.0) if total else 0.0\n",
    "    return pd.Series(out)\n",
    "\n",
    "def enu_row_summary(df: pd.DataFrame, eps: float = 1e-6, ignore_first_per_flight: bool = True):\n",
    "    needed = {\"E\",\"N\",\"U\"}\n",
    "    if not needed.issubset(df.columns):\n",
    "        raise KeyError(\"DataFrame must have E, N, U columns.\")\n",
    "    valid = df[[\"E\",\"N\",\"U\"]].notna().all(axis=1)\n",
    "    if ignore_first_per_flight and \"flight_id\" in df.columns:\n",
    "        first_idx = df.groupby(\"flight_id\").head(1).index\n",
    "        valid_ex = valid.copy(); valid_ex.loc[first_idx] = False\n",
    "    else:\n",
    "        valid_ex = valid\n",
    "    any_nz = valid_ex & (df[[\"E\",\"N\",\"U\"]].abs() > eps).any(axis=1)\n",
    "    all_nz = valid_ex & (df[[\"E\",\"N\",\"U\"]].abs() > eps).all(axis=1)\n",
    "    total = len(df); tot_valid = int(valid.sum())\n",
    "    return pd.Series({\n",
    "        \"total_rows\": total,\n",
    "        \"valid_rows\": tot_valid,\n",
    "        \"valid_pct\": (tot_valid/total*100.0) if total else 0.0,\n",
    "        \"any_nonzero_rows\": int(any_nz.sum()),\n",
    "        \"any_nonzero_pct_of_valid\": (int(any_nz.sum())/max(tot_valid,1)*100.0),\n",
    "        \"all_nonzero_rows\": int(all_nz.sum()),\n",
    "        \"all_nonzero_pct_of_valid\": (int(all_nz.sum())/max(tot_valid,1)*100.0),\n",
    "        \"ignored_first_rows\": int(valid.sum() - valid_ex.sum()) if ignore_first_per_flight and \"flight_id\" in df.columns else 0\n",
    "    })\n",
    "\n",
    "print(enu_counts(df_runs, eps=1e-6))\n",
    "print(enu_row_summary(df_runs, eps=1e-6, ignore_first_per_flight=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "81c0349e-24a3-46b1-ad17-95b17c37201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/b4y6wwhd0m18s4zqjd6p25mc0000gn/T/ipykernel_2896/1931338290.py:10: DeprecationWarning: is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead\n",
      "  if pd.api.types.is_period_dtype(df_runs[c]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow failed: ArrowKeyError('A type extension with name pandas.interval already defined')\n",
      "Saved Parquet with fastparquet → /Users/danielvillafuerte/weather-aware-trajectory-prediction/notebooks/data/processed/flights_nativecadence_enu_kinematics.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "out_path = Path(\"data/processed/flights_nativecadence_enu_kinematics.parquet\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 0) Make sure there are no Pandas Period dtypes (rare, but safe to handle)\n",
    "for c in df_runs.columns:\n",
    "    if pd.api.types.is_period_dtype(df_runs[c]):\n",
    "        df_runs[c] = df_runs[c].astype(str)\n",
    "\n",
    "saved = False\n",
    "\n",
    "# 1) Try pyarrow\n",
    "try:\n",
    "    import pyarrow as pa  # type: ignore\n",
    "    try:\n",
    "        pa.unregister_extension_type(\"pandas.period\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    df_runs.to_parquet(out_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"Saved Parquet with pyarrow → {out_path.resolve()}\")\n",
    "    saved = True\n",
    "except Exception as e:\n",
    "    print(\"pyarrow failed:\", repr(e))\n",
    "\n",
    "# 2) Fallback using fastparquet (install if missing)\n",
    "if not saved:\n",
    "    try:\n",
    "        import fastparquet  # type: ignore\n",
    "    except ImportError:\n",
    "        import sys, subprocess\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fastparquet\", \"--quiet\"])\n",
    "            import fastparquet  # noqa: F401\n",
    "            print(\"Installed fastparquet.\")\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't install fastparquet:\", repr(e))\n",
    "\n",
    "    try:\n",
    "        df_runs.to_parquet(out_path, index=False, engine=\"fastparquet\", compression=\"snappy\")\n",
    "        print(f\"Saved Parquet with fastparquet → {out_path.resolve()}\")\n",
    "        saved = True\n",
    "    except Exception as e:\n",
    "        print(\"fastparquet failed:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e54466-2a04-429f-a6e6-0e0507d674ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
